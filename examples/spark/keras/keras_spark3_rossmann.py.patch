30c30
< parser = argparse.ArgumentParser(description='Keras Spark Rossmann Run Example',
---
> parser = argparse.ArgumentParser(description='Keras Spark3 Rossmann Run Example',
36c36
< parser.add_argument('--training-master',
---
> parser.add_argument('--training-master', default='local-cluster[2,1,1024]',
60a61,63
>     # Location of discovery script on local filesystem.
>     DISCOVERY_SCRIPT = 'get_gpu_resources.sh'
> 
63a67,72
>     # Whether to infer on GPU.
>     GPU_INFERENCE_ENABLED = False
> 
>     # Cluster for GPU inference.
>     GPU_INFERENCE_CLUSTER = 'local-cluster[2,1,1024]'  # or 'spark://hostname:7077'
> 
396a406
>         from horovod.spark.task import get_available_devices
410,411c420,422
<         if gpus:
<             gpu = gpus[hvd.local_rank()]
---
>         gpu_ids = get_available_devices()
>         if gpus and gpu_ids:
>             gpu = gpus[int(gpu_ids[0])]
495a507,531
>     def set_gpu_conf(conf):
>         # This config will change depending on your cluster setup.
>         #
>         # 1. Standalone Cluster
>         # - Must configure spark.worker.* configs as below.
>         #
>         # 2. YARN
>         # - Requires YARN 3.1 or higher to support GPUs
>         # - Cluster should be configured to have isolation on so that
>         #   multiple executors don’t see the same GPU on the same host.
>         # - If you don’t have isolation then you would require a different discovery script
>         #   or other way to make sure that 2 executors don’t try to use same GPU.
>         #
>         # 3. Kubernetes
>         # - Requires GPU support and isolation.
>         # - Add conf.set(“spark.executor.resource.gpu.discoveryScript”, DISCOVERY_SCRIPT)
>         # - Add conf.set(“spark.executor.resource.gpu.vendor”, “nvidia.com”)
>         conf = conf.set("spark.test.home", os.environ.get('SPARK_HOME'))
>         conf = conf.set("spark.worker.resource.gpu.discoveryScript", DISCOVERY_SCRIPT)
>         conf = conf.set("spark.worker.resource.gpu.amount", 1)
>         conf = conf.set("spark.task.resource.gpu.amount", "1")
>         conf = conf.set("spark.executor.resource.gpu.amount", "1")
>         return conf
> 
> 
499a536
>     conf = set_gpu_conf(conf)
530,531c567,575
<     if args.processing_master:
<         conf.setMaster(args.processing_master)
---
> 
>     if GPU_INFERENCE_ENABLED:
>         if GPU_INFERENCE_CLUSTER:
>             conf.setMaster(GPU_INFERENCE_CLUSTER)
>         conf = set_gpu_conf(conf)
>     else:
>         if args.processing_master:
>             conf.setMaster(args.processing_master)
> 
542,545c586,598
<             # Do not use GPUs for prediction, use single CPU core per task.
<             tf.config.set_visible_devices([], 'GPU')
<             tf.config.threading.set_inter_op_parallelism_threads(1)
<             tf.config.threading.set_intra_op_parallelism_threads(1)
---
>             if GPU_INFERENCE_ENABLED:
>                 from pyspark import TaskContext
>                 gpus = tf.config.list_physical_devices('GPU')
>                 gpu_id = TaskContext.get().resources()['gpu'].addresses[0]
>                 if gpus:
>                     gpu = gpus[int(gpu_id)]
>                     tf.config.experimental.set_memory_growth(gpu, True)
>                     tf.config.experimental.set_visible_devices(gpu, 'GPU')
>             else:
>                 # Do not use GPUs for prediction, use single CPU core per task.
>                 tf.config.set_visible_devices([], 'GPU')
>                 tf.config.threading.set_inter_op_parallelism_threads(1)
>                 tf.config.threading.set_intra_op_parallelism_threads(1)
