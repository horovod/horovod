32a33
> DISCOVERY_SCRIPT = 'get_gpu_resources.sh'
42c43
< TRAINING_CLUSTER = None  # or 'spark://hostname:7077'
---
> TRAINING_CLUSTER = 'local-cluster[2,1,1024]'  # or 'spark://hostname:7077'
57a59,64
> # Whether to infer on GPU.
> GPU_INFERENCE_ENABLED = False
> 
> # Cluster for GPU inference.
> GPU_INFERENCE_CLUSTER = 'local-cluster[2,1,1024]'  # or 'spark://hostname:7077'
> 
391a399
>     from horovod.spark.task import get_available_devices
406c414
<     config.gpu_options.visible_device_list = str(hvd.local_rank())
---
>     config.gpu_options.visible_device_list = get_available_devices()[0]
488a497,521
> def set_gpu_conf(conf):
>     # This config will change depending on your cluster setup.
>     #
>     # 1. Standalone Cluster
>     # - Must configure spark.worker.* configs as below.
>     #
>     # 2. YARN
>     # - Requires YARN 3.1 or higher to support GPUs
>     # - Cluster should be configured to have isolation on so that
>     #   multiple executors don’t see the same GPU on the same host.
>     # - If you don’t have isolation then you would require a different discovery script
>     #   or other way to make sure that 2 executors don’t try to use same GPU.
>     #
>     # 3. Kubernetes
>     # - Requires GPU support and isolation.
>     # - Add conf.set(“spark.executor.resource.gpu.discoveryScript”, DISCOVERY_SCRIPT)
>     # - Add conf.set(“spark.executor.resource.gpu.vendor”, “nvidia.com”)
>     conf = conf.set("spark.test.home", os.environ.get('SPARK_HOME'))
>     conf = conf.set("spark.worker.resource.gpu.discoveryScript", DISCOVERY_SCRIPT)
>     conf = conf.set("spark.worker.resource.gpu.amount", 1)
>     conf = conf.set("spark.task.resource.gpu.amount", "1")
>     conf = conf.set("spark.executor.resource.gpu.amount", "1")
>     return conf
> 
> 
492a526
> conf = set_gpu_conf(conf)
521,522c555,563
< if LIGHT_PROCESSING_CLUSTER:
<     conf.setMaster(LIGHT_PROCESSING_CLUSTER)
---
> 
> if GPU_INFERENCE_ENABLED:
>     if GPU_INFERENCE_CLUSTER:
>         conf.setMaster(GPU_INFERENCE_CLUSTER)
>     conf = set_gpu_conf(conf)
> else:
>     if LIGHT_PROCESSING_CLUSTER:
>         conf.setMaster(LIGHT_PROCESSING_CLUSTER)
> 
532,536c573,584
<         # Do not use GPUs for prediction, use single CPU core per task.
<         config = tf.ConfigProto(device_count={'GPU': 0})
<         config.inter_op_parallelism_threads = 1
<         config.intra_op_parallelism_threads = 1
<         K.set_session(tf.Session(config=config))
---
>         if GPU_INFERENCE_ENABLED:
>             from pyspark import TaskContext
>             config = tf.ConfigProto()
>             config.gpu_options.allow_growth = True
>             config.gpu_options.visible_device_list = TaskContext.get().resources()['gpu'].addresses[0]
>             K.set_session(tf.Session(config=config))
>         else:
>             # Do not use GPUs for prediction, use single CPU core per task.
>             config = tf.ConfigProto(device_count={'GPU': 0})
>             config.inter_op_parallelism_threads = 1
>             config.intra_op_parallelism_threads = 1
>             K.set_session(tf.Session(config=config))
552a601
> # Submit a Spark job to do inference. Horovod framework is not involved here.
